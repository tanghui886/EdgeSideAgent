import torch
from PIL import Image
import base64
from io import BytesIO
import uvicorn
from fastapi import FastAPI,HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List,Optional,Union
from transformers import Qwen2_5_VLForConditionalGeneration,AutoProcessor
import time
import uuid
import os
import argparse

# ========================
#python runModel.py --modelPath /home/modelscope/ai/model/Qwen2.5-VL-7B-Instruct --modelName Qwen2.5-VL-7B-Instruct --port 8002
# âœ… è§£æå‘½ä»¤è¡Œå‚æ•°
parser = argparse.ArgumentParser(description="Model API Server")
parser.add_argument("--modelPath", type=str, required=True, help="modelPath to the model directory")
parser.add_argument("--modelName", type=str, required=True, help="modelName to the model name")
parser.add_argument("--port", type=int, default=8001, help="Port to run the server on (default: 8001)")
parser.add_argument("--dtype", type=str, default=torch.bfloat16, help="torch_dtype (default: torch.bfloat16)")
args = parser.parse_args()
# ========================
model_path=args.modelPath
model_name=args.modelName
port=args.port
dtype=args.dtype
print(f"ğŸ§  æ­£åœ¨åŠ è½½ {model_name}ï¼ˆTransformers + ROCm GPUï¼‰...")
# æ£€æµ‹è®¾å¤‡
device="cuda" if torch.cuda.is_available() else "cpu"
print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")

try:
    from transformers import AutoModelForCausalLM
    model=Qwen2_5_VLForConditionalGeneration.from_pretrained(
        model_path,
        attn_implementation="eager",  # ä½¿ç”¨eageræ¨¡å¼ï¼Œå…¼å®¹æ€§æ›´å¥½ flash_attention_2 flash_attention_3
        torch_dtype=dtype,
        load_in_4bit=False,
        device_map="auto",
        trust_remote_code=True,
    )

    processor=AutoProcessor.from_pretrained(
        model_path,
        trust_remote_code=True
    )

    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")

except Exception as e:
    print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    try:
        model=AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None,
            load_in_4bit=False,
            trust_remote_code=True,
        )
        if not torch.cuda.is_available():
            model=model.to(device)

        processor=AutoProcessor.from_pretrained(model_path,trust_remote_code=True)
        print("âœ… æ¨¡å‹ä½¿ç”¨å¤‡ç”¨æ–¹å¼åŠ è½½å®Œæˆï¼")
    except Exception as e2:
        print(f"âŒ å¤‡ç”¨åŠ è½½æ–¹å¼ä¹Ÿå¤±è´¥: {e2}")
        raise RuntimeError("æ— æ³•åŠ è½½æ¨¡å‹")

# ========================
# ğŸŒ FastAPI åˆå§‹åŒ–
# ========================
app=FastAPI(title="Qwen2.5-VL API Server (ROCm Native)")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ========================
# ğŸ“¥ è¯·æ±‚ä½“å®šä¹‰ï¼ˆå…¼å®¹ OpenAI æ ¼å¼ï¼‰
# ========================
class ImageUrlContent(BaseModel):
    url: str


class ImageUrlPart(BaseModel):
    type: str="image_url"
    image_url: ImageUrlContent


class TextPart(BaseModel):
    type: str="text"
    text: str


ContentPart=Union[TextPart,ImageUrlPart]


class Message(BaseModel):
    role: str
    content: Union[str,List[ContentPart]]


class ChatCompletionRequest(BaseModel):
    model: str=model_name
    messages: List[Message]
    max_tokens: Optional[int]=512
    temperature: Optional[float]=0.7
    stream: Optional[bool]=False


# ========================
# ğŸ“¤ å“åº”ä½“å®šä¹‰
# ========================
class ChatMessage(BaseModel):
    role: str
    content: str


class Choice(BaseModel):
    index: int=0
    message: ChatMessage
    finish_reason: str="stop"


class Usage(BaseModel):
    prompt_tokens: int=0
    completion_tokens: int=0
    total_tokens: int=0


class ChatCompletionResponse(BaseModel):
    id: str
    object: str="chat.completion"
    created: int
    model: str
    choices: List[Choice]
    usage: Usage


# ========================
# ğŸ–¼ï¸ å·¥å…·å‡½æ•°ï¼šBase64 â†’ PIL Image
# ========================
def base64_to_image(b64_str: str) -> Image.Image:
    try:
        if b64_str.startswith("data:image"):
            b64_str=b64_str.split(",")[1]
        image_data=base64.b64decode(b64_str)
        return Image.open(BytesIO(image_data)).convert("RGB")
    except Exception as e:
        raise HTTPException(status_code=400,detail=f"Invalid image base64: {str(e)}")


# ========================
# ğŸš€ æ ¸å¿ƒæ¨ç†å‡½æ•°
# ========================
def generate_response(messages: List[Message], max_tokens: int, temperature: float):
    try:
        # æ„å»ºæ¶ˆæ¯å†…å®¹
        text_content = ""
        images = []

        for msg in messages:
            content = msg.content
            if isinstance(content, str):
                text_content += content + "\n"
            elif isinstance(content, list):
                for part in content:
                    if isinstance(part, TextPart):
                        text_content += part.text + "\n"
                    elif isinstance(part, ImageUrlPart):
                        img = base64_to_image(part.image_url.url)
                        images.append(img)

        # å‡†å¤‡æ¨¡å‹è¾“å…¥
        messages_for_model = [
            {
                "role": "user",
                "content": []
            }
        ]

        # æ·»åŠ å›¾åƒ
        if images:
            temp_image_path = f"temp_image_{uuid.uuid4().hex}.jpg"
            images[0].save(temp_image_path, quality=95, optimize=True)
            messages_for_model[0]["content"].append(
                {"type": "image", "image": temp_image_path}
            )

        # æ·»åŠ æ–‡æœ¬
        messages_for_model[0]["content"].append(
            {"type": "text", "text": text_content.strip()}
        )

        # ä½¿ç”¨processoråº”ç”¨èŠå¤©æ¨¡æ¿
        text = processor.apply_chat_template(
            messages_for_model,
            tokenize=False,
            add_generation_prompt=True
        )

        # å¤„ç†è§†è§‰ä¿¡æ¯
        try:
            from qwen_vl_utils import process_vision_info
            image_inputs, video_inputs = process_vision_info(messages_for_model)
        except ImportError:
            # å¦‚æœæ²¡æœ‰ qwen_vl_utilsï¼Œä½¿ç”¨å¤‡ç”¨æ–¹å¼
            image_inputs = [images[0]] if images else None
            video_inputs = None

        # å‡†å¤‡è¾“å…¥
        inputs = processor(
            text=[text],
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt",
        ).to(model.device)

        input_ids = inputs.input_ids  # ä¿å­˜è¾“å…¥ token ids

        # ç”Ÿæˆå“åº”
        with torch.no_grad():
            generated_ids = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                temperature=temperature,
                do_sample=temperature > 0,
                pad_token_id=processor.tokenizer.pad_token_id or processor.tokenizer.eos_token_id,
            )

        # è£å‰ªæ‰è¾“å…¥éƒ¨åˆ†ï¼Œåªä¿ç•™ç”Ÿæˆéƒ¨åˆ†
        generated_ids_trimmed = [
            out_ids[len(in_ids):] for in_ids, out_ids in zip(input_ids, generated_ids)
        ]

        output_text = processor.batch_decode(
            generated_ids_trimmed,
            skip_special_tokens=True,
            clean_up_tokenization_spaces=False
        )[0]

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if 'temp_image_path' in locals() and os.path.exists(temp_image_path):
            os.remove(temp_image_path)

        # âœ… è¿”å›ä¸‰å…ƒç»„ï¼šæ–‡æœ¬ã€è¾“å…¥tokenæ•°ã€è¾“å‡ºtokenæ•°
        prompt_token_count = input_ids.shape[1]  # è¾“å…¥åºåˆ—é•¿åº¦
        completion_token_count = generated_ids_trimmed[0].shape[0]  # è¾“å‡ºåºåˆ—é•¿åº¦

        return output_text, prompt_token_count, completion_token_count

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Inference error: {str(e)}")


# ========================
# ğŸŒ API ç«¯ç‚¹ï¼š/v1/chat/completions
# ========================
@app.post("/v1/chat/completions", response_model=ChatCompletionResponse)
async def chat_completions(request: ChatCompletionRequest):
    if request.stream:
        raise HTTPException(status_code=400, detail="Streaming not supported yet.")

    start_time = time.time()

    # âœ… ç°åœ¨ generate_response è¿”å›ä¸‰ä¸ªå€¼
    response_text, prompt_tokens, completion_tokens = generate_response(
        request.messages, request.max_tokens, request.temperature
    )

    end_time = time.time()
    total_time = end_time - start_time
    tps = completion_tokens / total_time if total_time > 0 else 0

    # ğŸ–¨ï¸ æ‰“å°æ€§èƒ½ç»Ÿè®¡
    print(f"\nğŸ“Š [æ€§èƒ½ç»Ÿè®¡]")
    print(f"ğŸ“¥ è¾“å…¥ tokens: {prompt_tokens}")
    print(f"ğŸ“¤ è¾“å‡º tokens: {completion_tokens}")
    print(f"â±ï¸  ç”Ÿæˆé€Ÿåº¦: {tps:.2f} tokens/s")
    print(f"â³ æ€»è€—æ—¶: {total_time:.3f} ç§’\n")

    total_tokens = prompt_tokens + completion_tokens

    return ChatCompletionResponse(
        id=f"chatcmpl-{uuid.uuid4().hex}",
        created=int(time.time()),
        model=request.model,
        choices=[
            Choice(
                message=ChatMessage(role="assistant", content=response_text)
            )
        ],
        usage=Usage(
            prompt_tokens=prompt_tokens,
            completion_tokens=completion_tokens,
            total_tokens=total_tokens
        )
    )


# å¥åº·æ£€æŸ¥ç«¯ç‚¹
@app.get("/health")
async def health_check():
    return {
        "status":"healthy",
        "device":str(model.device),
        "dtype":str(model.dtype),
        "model":model_name
    }


# ========================
# ğŸ å¯åŠ¨æœåŠ¡
# ========================
if __name__ == "__main__":
    print(f"ğŸš€ {model_name} API Server å¯åŠ¨ä¸­...")
    print(f"ğŸ–¥ï¸  è®¾å¤‡: {device}")
    print(f"ğŸ“Š ç²¾åº¦: {model.dtype}")
    print(f"ğŸŒ äº¤äº’æ–‡æ¡£: http://localhost:{port}/docs")
    print(f"ğŸ“Œ è¯·æ±‚åœ°å€: POST http://localhost:{port}/v1/chat/completions")
    print(f"ğŸ¥ å¥åº·æ£€æŸ¥: GET http://localhost:{port}/health")

    uvicorn.run(app,host="0.0.0.0",port=port)